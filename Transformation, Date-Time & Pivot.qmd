---
title: "Challenge_2: Data Transformation(2), Pivot and Date-Time Data"
author: "Sagnik Chand"
description: "This is our second weekly challenge where we need to perform data transformation using Pivot and Date-Time functions"
date: "09/26/2023"
format:
  html:
    df-print: paged
    css: "styles.css"
    embed-resources: true
    self-contained-math: true
categories:
  - weekly_challenges
  - challenge_2
editor: 
  markdown: 
    wrap: sentence
---

**Make sure you change the author's name in the above YAML header.**

## Setup

If you have not installed the following packages, please install them before loading them.

```{r}
#| label: setup
#| warning: false
#| message: false

library(tidyverse)
library(readxl)
library(haven) #for loading other datafiles (SAS, STATA, SPSS, etc.)
library(stringr) # if you have not installed this package, please install it.
library(lubridate)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## Challenge Overview

Building on the lectures in week#3 and week#4, we will continually practice the skills of different transformation functions with Challenge_2.
In addition, we will explore the data more by conducting practices with pivoting data and dealing with date-time data.

There will be coding components and writing components.
Please read the instructions for each part and complete your challenges.

## Datasets

There are four datasets provided in this challenge.
Please download the following dataset files from Canvas or Google Classroom and save them to a folder within your project working directory (i.e.: "yourworkingdiectory_data").
If you don't have a folder to store the datasets, please create one.

-   ESS_5.dta (Part 1) ⭐
-   p5v2018.sav (Part 1)⭐
-   austrlian_data.csv (Part 3)⭐
-   FedFundsRate.csv (Part 4)⭐

Find the `_data` folder, then use the correct R command to read the datasets.

## Part 1(Required). Depending on the data you chose in Challenge#1 (ESS_5 or Polity V), please use that data to complete the following tasks

## **If you are using the ESS_5 Data:**

1.  **Read the dataset and keep the first 39 columns.**

```{r}
# Reading the ESS_5.dta data set and selecting the required columns.

Ess <- read_dta("ESS_5.dta")
Ess <- Ess %>% 
  select(1:39)
head(Ess)
```

2.  **Conduct the following transformation for the data by using mutate() and other related functions :**

    \(1\) Create a new column named "YearOfBirth" using the information in the "age" column.

    \(2\) Create a new column named "adult" using the information in the "age" column.

    \(3\) Recode the "commonlaw" column: if the value is 0, recode it as "non-common-law"; if the value is 1, recode it as "common-law".

    \(4\) Recode the "vote" column: if the value is 3, recode it as 1; if the value is smaller than 3, recode it as 0.
    Make sure to exclude the NAs.

    \(5\) Move the column "YearOfBirth", "adult," "commonlaw" and "vote" right after the "essround" column (the 2nd column in order).

    \(6\) Answer the question: What is the data type of the "commonlaw" column before and after recoding?
    And what is the data type of the "vote" column before and after recoding?

    Ans: Ans: The data type for the "commonlaw" column was double before and it changed to character after re-coding.
    Similarly, the data type for the "vote" column was haven labelled variable before and it changed to character after re-coding.

```{r}
# Creating a new column called YearOfBirth.
Ess <- Ess %>% 
  mutate( YearOfBirth = 2023 - Ess$age) %>% 
  relocate( YearOfBirth, .before = age)

# Creating a new column called Adult.
Ess <- Ess %>% 
  mutate(Adult = Ess$age >= 18) %>% 
  relocate(Adult, .after = age)

# Re-coding the commonlaw column.
Ess <- Ess %>% 
  mutate(commonlaw = recode(commonlaw, "0" = "non-common-law", "1" = "common-law")) %>% 
  relocate(commonlaw, .after = Adult)

# Re-coding and filtering the vote column.
Ess <- Ess %>% 
  filter(!is.na(vote)) %>% 
  mutate(vote = case_when( vote == 3 ~ "1",
                           vote < 3 ~ "0"
                           )) %>% 
  relocate(vote, .after = Adult)

# Relocating YearOfBirth, Adult, commonlaw, and vote columns.
Ess <- Ess %>% 
  relocate(c(YearOfBirth, Adult, commonlaw, vote), .after = essround)

head(Ess)
```

## **If you are using the Polity V Data:**

1.  **Read the dataset and keep the first 11 columns.**

```{r}
#Type your code here
```

2.  **Conduct the following transformation for the data by using mutate() and other related functions :**

    \(1\) Create a new column named "North America" using the information in the "country" column.
    Note: "United States," "Mexico," or "Canada" are the countries in North America.
    In the new "North America" column, if a country is one of the above three countries, it should be coded as 1, otherwise as 0.

    \(2\) Recode the "democ" column: if the value is 10, recode it as "Well-Functioning Democracy"; if the value is greater than 0 and smaller than 10, recode it as "Either-Autocracy-or-Democracy"; if the value is 0, recode it as "Non-democracy"; if the value is one of the following negative integers (-88, -77, and -66), recode it as "Special-Cases."

    \(3\) Move the column "North America" and "democ" right before the "year" column (the 6th column in order).

    \(4\) Answer the question: What is the data type of the "North America" column?
    What is the data type of the "democ" column before and after recoding?

```{r}
#Type your code here
```

## Part 2. Generate your own Data

1.  **Generate an untidy data that includes 10 rows and 10 columns. In this dataset, column names are not names of variables but a value of a variable.**

    \*Note: do not ask ChatGPT to generate a dataframe for you.
    I have already checked the possible questions and answers generated by AI.

```{r}
# Generating a wide data frame.

df <- data.frame(
  ID = 1:10,
  Sex = c("M", "M", "F", "M", "M", "F", "F", "F", "M", "F"),
  Day_1 = round(runif(10, min = 70, max = 90), digits = 2),
  Day_2 = round(runif(10, min = 65, max = 87), digits = 2),
  Day_3 = round(runif(10, min = 67.25, max = 99),digits = 2),
  Day_4 = round(runif(10, min = 60, max = 95),digits = 2),
  Day_5 = round(runif(10, min = 65, max = 87),digits = 2),
  Day_6 = round(runif(10, min = 67.25, max = 99),digits = 2),
  Day_7 = round(runif(10, min = 67.25, max = 99),digits = 2),
  Day_8 = round(runif(10, min = 67.25, max = 99),digits = 2)
)
```

2.  **Use the correct pivot command to convert the data to tidy data.**

```{r}
# Using pivot_longer() to convert a wide data frame into a long data frame.

df_long <- df %>% 
  pivot_longer(3:10, names_to = "Days", values_to = "temparature")
View(df_long)
```

3.  **Generate an untidy data that includes 10 rows and 5 columns. In this dataset, an observation is scattered across multiple rows.**

```{r}
# Generating a long data frame.

df_2 <- data.frame(
  ID = c("1","1","2","2","3","3","4","4","5","5"),
  Var = c("Score1","Score2","Score1","Score2","Score1","Score2","Score1","Score2","Score1","Score2"),
  Eng = sample(50:100, 10, replace = F),
  Jap = sample(50:100, 10, replace = F),
  Chi = sample(50:100, 10, replace = F)
)
```

4.  **Use the correct pivot command to convert the data to tidy data.**

```{r}
# Using pivot_wider() to convert long data frame to wide data frame.

df2_wide <- df_2 %>% 
  pivot_wider(names_from = 2, values_from = 3:5)

View(df2_wide)
```

## Part 3. The Australian Data

This is another tabular data source published by the [Australian Bureau of Statistics](https://www.abs.gov.au/) that requires a decent amount of cleaning.
In 2017, Australia conducted a postal survey to gauge citizens' opinions towards same sex marriage: "Should the law be changed to allow same-sex couples to marry?" All Australian citizens are required to vote in elections, so citizens could respond in one of four ways: vote yes, vote no, vote in an unclear way(illegible), or fail to vote.
(See the "Explanatory Notes" sheet for more details.)

I have already cleaned up the data for you and you can directly import it.
We will come back to clean and process the original "messy" data after we learn some string functions in the later weeks.

1.  **Read the dataset "australian_data.csv":**

```{r}
# Importing the data-set and viewing it.

aus <- read_csv("australian_data.csv")
aus <- aus %>% 
  select(-...1)
View(aus)
```

-   **Data Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.**

    ```{r}
    # reading the dimensions of the data-set.
    dim(aus)

    # Getting to know the data-set and about the rows and columns.
    head(aus)
    str(aus)

    # Transforming the data-set into a long format.
    aus_long <- aus %>% 
      pivot_longer(2:5, names_to = "Opinion", values_to = "Votes")

    View(aus_long)
    ```

    \(1\) What is the dimension of the data (# of rows and columns)?

    Ans: 150 rows and 7 columns.

    \(2\) What do the rows and columns mean in this data?

    Ans: The rows in this data-set are individual observations grouped by districts and the columns define various attributes about how the respondents in each district voted with regard to approving the 'same sex marriage law' in Australia, again grouped within their particular divisions.

    \(3\) What is the unit of observation?
    In other words, what does each case mean in this data?

    Ans: Unit of observation is a district in a given division.

    \(4\) According to the lecture, is this a "tidy" data?
    Why?

    Ans: For most of the survey data, each of the rows represents a unique respondent (as the unit of observation). In this type of data, survey questions are listed as different columns (variables), and all possible responses to questions are the values of cells. In this case, "Yes, No, illegible, and No Response" will be filled in the cells under the survey question on the "attitudes towards same-sex marriage". In this scenario, we definitely want to pivot_longer by converting the columns of Yes, No, illegible, and No Response" to the values of a column (survey question). However, the Australian data is aggregate data, so the situation becomes complicated. With the original data, each row represents a unique district within a division (meets the requirement of tidy data). However, it is debatable whether we treat the columns "Yes, No, Illegible, No Response" as separate variables (because they should be treated as four different possible values in the case of the individual-level survey data). If we choose to reshape it by pivot_longer and create a column called "Response", each of the districts will have four different rows associated with different "responses (Yes, No, Illegible, No Response)". In the reshaped data, each row does not represent the unique district within a division anymore. Think of it another way: each row now represents a unique group of people in the given division with a specific attitude/response to same-sex marriage.

    \(5\) If this is not a tidy data, please use the necessary commands to make it "tidy".

    Ans: We can transform this data-set from wide format to a longer data-set by using the pivot_longer() function.
    This way each row will have just a single piece of observation in it.

-   **Data Transformation: use necessary commands and codes and answer the following questions. If you reshape the data in the previous step, please work on the reshaped data.**

    ```{r}
    # Reviewing the data sets.
    head(aus_long)
    head(aus)

    # Calculating number of unique districts in the data set.
    length(unique(aus_long$District))
    n_distinct(aus_long$District)

    # Calculating number of unique divisions in the data set.
    length(unique(aus_long$Division))
    n_distinct(aus_long$Division)

    # Mutating a total population column into the data set.
    aus_long <- aus_long %>% 
      group_by(District) %>% 
      mutate(Population = sum(Votes))
      
    # Mutating the district turnout column with the data set.
    aus_long <- aus_long %>%
      mutate(Population = sum(Votes)) %>% 
      filter(Opinion != "No Response") %>%
      mutate(Total_votes = sum(Votes)) %>% 
      mutate(Dist_Turnout = (Total_votes/Population)*100)

    # Calculating the total number of people who are for and against same sex marriage in Australia.
    aus_long %>% 
      group_by(Opinion) %>% 
      summarise( Total_peeps = sum(Votes))

    # Calculating which *district* has ***most people*** supporting the policy.
    aus_long %>% 
      filter(Opinion == "Yes")%>% 
      select(District,Votes) %>%
      arrange(desc(Votes))

    # Summarizing which *division* has the highest approval rate (% of "yes" in the total votes).
      Avg_apprv  <- aus_long %>% 
      filter(Opinion == "Yes") %>%
      group_by(Division) %>%
      summarise(div_apprv = (sum(Votes)/sum(Total_votes))*100) %>% 
      arrange(desc(div_apprv))

    # Calculating the average rate of approval 
      Avg_apprv %>% 
      summarise( Avg_apprv = mean(div_apprv))

    ```

    \(1\) How many districts and divisions are in the data?

    Ans: There are '150' districts and '8' different divisions in this data-set.

    \(2\) Use mutate() to create a new column "district turnout(%)".
    This column should be the voting turnout in a given district, or the proportion of people cast votes (yes, no, and illegible) in the total population of a district.

    \(3\) please use summarise() to estimate the following questions:

    -   In total, how many people support same-sex marriage in Australia, and how many people oppose it?

        Ans: In total '7817247' people support same-sex marriage and '4873987' people are against it.

    -   Which *district* has the ***most people*** supporting the policy, and how many?

        Ans: 'Canberra(d)' has '89590' people supporting the policy.
        That is the highest among all the districts.

    -   Which *division* has the highest approval rate (% of "yes" in the total casted votes)?
        And what is the average approval rate at the *division level?*

        Ans: The 'Australian Capital Territory Divisions' has the highest approval rate of '73.873 %' and the average approval rate at the division level is '63.304 %'.

    \(4\) Suppose that we wanted to add additional division-level characteristics (new columns/variables) to the data and use these new characteristics to build a statistical model (such as an ordinal logistic regression) to predict people's voting response.
    Is the current data shape still good?
    If not, how should we change it?

    Ans: We can us the pivot_longer() function to transform the current data set in to a long data format such that each row represents a single observation with regard to the type of vote, again segregated into districts and divisions.
    This long format data set will let us perform integral statistical modelling efficiently.

## Part 4. The Marco-economic Data

This data set runs from July 1954 to March 2017, and includes daily macroeconomic indicators related to the *effective federal funds rate* - or [the interest rate at which banks lend money to each other](https://en.wikipedia.org/wiki/Federal_funds_rate) in order to meet mandated reserve requirements.

1.  **Read the dataset "FedFundsRate.csv":**

```{r}
  # Reading and importing the data set.
  Fed <- read_csv("FedFundsRate.csv")
  head(Fed)

```

2.  **Data Description: Please use the necessary commands and codes and briefly describe this data with a short writing paragraph answering the following questions.**

```{r}
  # Applying functions to learn about the structure of this data frame.
  str(Fed)
  dim(Fed)
    
```

```         
\(1\) What is the dimension of the data (# of rows and columns)?
Ans: There are '904' rows and '10' columns in this data set.

\(2\) What do the rows and columns mean in this data?
Ans: This data set contains information about the daily macronomic indicators related to effective federal funds rate
starting from July, 1954. Each row/observation contains the macronomic indicators for a particular month in that year and the columns represent various attributes related to the federal funds rate such as Inflation Rate, Real GDP, Unemployment Rate, Effective Federal Funds Rate, and so on.

\(3\) What is the unit of observation? In other words, what does each case mean in this data?
Ans: Each case contains the macronomic indicators for a particular month in that year i.e it explains various attributes related to that month's federal funds rate. This indicates that the unit of observation are individual months segregated by years.
```

3.  **Generating a date column:**

    Notice that the year, month, and day are three different columns.
    We will first have to use a string function called "str_c()" from the "stringr" library to combine these three columns into one "date" column.
    Please revise the following commands

    ```{r}
    # creating a new column called Date and relocating it to the first column.
    Fed_V1 <- Fed %>% 
      mutate( Date = str_c(Year, Month, Day, sep = "-")) %>% 
      relocate(Date, .before = Year)

    head(Fed_V1)

    ```

4.  **Move the new created "date" column to the beginning as the first column of the data.**

5.  **What is the data type of the new "date" column?**

    ```{r}
    # Finding out the data type of column date.
    class(Fed_V1$Date)
    ```

6.  **Transform the "date" column to a \<date\> data.**

    ```{r}
    # Converting Date column's data type to \<date>\.
    Fed_V1 <- Fed_V1 %>% 
      mutate( Date = as_date(Fed_V1$Date))
    Fed_V1 <- Fed_V1 %>% 
      mutate( Date = ymd(Fed_V1$Date))

    class(Fed_V1$Date)
    ```

7.  **Conduct following statistics:**

    ```{r}
    # Figuring out the dates with highest and lowest unemployment rates.
    unemp_rate <- Fed_V1 %>% 
      select(c(Date,`Unemployment Rate`)) %>% 
      filter(!is.na(Fed_V1$`Unemployment Rate`)) %>% 
      arrange(desc(`Unemployment Rate`))
    head(unemp_rate)
    tail(unemp_rate)

    ```

    \(1\) On which *date* is the highest unemployment rate?
    and the lowest?

    Ans: The highest unemployment rate is on '01-11-1982' and the lowest unemployment rate is on '01-12-1968'.

    \(2\) (Optional) Which *decade* has the highest average unemployment rate?

    Ans: 1980s has the highest average unemployment rate.
    The rate of unemployment is '7.27'%.

    Here is a template for you to create a decade column to allow you to group the data by decade.
    You can use it for the optional question in Challenge#1:

    ```{r}
    #fed_rates <- fed_rates |>
    #  mutate(Decade = cut(Year, breaks = seq(1954, 2017, by = 10), labels = format(seq(1954, 2017, by = 10), format = "%Y")))

    # Creating a column for decade.
    Fed_V1 <- Fed_V1 %>% 
      mutate( Decade = floor((Year%/%10)*10)) %>% 
      relocate(Decade, .after = Date)

    # Calculating the average unemployment rate for decades.
    Fed_V1 %>% 
      filter(!is.na(Fed_V1$`Unemployment Rate`)) %>% 
      group_by(Decade) %>% 
      summarise(avg_unemp = mean(`Unemployment Rate`)) %>% 
      arrange(desc(avg_unemp))
      
    ##Note: the cut() a baseR function that we don't generally use. Basically, it allows us divides the range of Year into intervals and codes the values in Year according to which interval (1954 and 2017) they fall; the break argument specifies how we segmate the sequence of Year (by a decade)
    ```
